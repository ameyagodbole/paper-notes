{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Web as a Knowledge-base for Answering Complex Questions\n",
    "\n",
    "Alon Talmor, Jonathan Berant (NAACL 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Past work\n",
    "\n",
    "* Neural RC Methods:\n",
    "    * Near human-level performance\n",
    "    * They mostly excel at matching questions to local contexts, but struggle with questions that require reasoning\n",
    "    * RC assumes documents with the information relevant for the answer are available\n",
    "* QA\n",
    "    * *Compositionality*: Questions are translated to compositional programs that encode a sequence of actions for finding the answer in a knowledge-base (KB)\n",
    "    * This reliance on a manually-curated KB has limited the coverage and applicability of semantic parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributions:\n",
    "\n",
    "* framework for answering complex questions through question decomposition\n",
    "* sequence-to-sequence model for question decomposition\n",
    "* [dataset](https://www.tau-nlp.org/compwebq) of 34,689 examples of complex and broad questions, along with answers, web snippets, and SPARQL queries\n",
    "\n",
    "### Assumption\n",
    "\n",
    "**answering simple questions can be achieved by combining a search engine with a RC model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem formulation\n",
    "\n",
    "The authors assume a pretrained, blackbox model ${\\rm S{\\scriptsize IMP}QA}$ which works by \n",
    "\n",
    "1. submitting the question to a search engine that retrieves web snippets\n",
    "2. using an RC model to extract answer from the snippets\n",
    "\n",
    "The proposed model decomposes a question *q* into a computation tree *t*. A computation tree has (paraphrased) strings as leaves and functions at inner nodes. An answer is computed by recursively applying the function at the root to its child subtrees (function at leaf is the identity). Available functions:\n",
    "\n",
    "1. ${\\rm S{\\scriptsize IMP}QA}(.)$: Takes a string argument and returns the answer\n",
    "2. ${\\rm C{\\scriptsize OMP}}(.,.)$: Takes a string with a variable VAR and a set of of answers.\n",
    "$${\\rm C{\\scriptsize OMP}}(q, \\mathcal{A}) = \\cup_{a\\in\\mathcal{A}} {\\rm S{\\scriptsize IMP}QA}(q/a)$$ where q/a denotes the string produced when replacing VAR in q with a\n",
    "3. ${\\rm C{\\scriptsize ONJ}}(.,.)$: Takes two sets and returns their intersection. Other set operations can be defined analogously.\n",
    "4. ${\\rm A{\\scriptsize DD}}(·,·)$: Takes two singleton sets of numbers and returns a set with their addition. Similar mathematical operations can be defined analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset collection\n",
    "\n",
    "Based on and extends [${\\rm W{\\scriptsize EB}Q{\\scriptsize UESTIONS}SP}$](http://aka.ms/WebQSP) (Yih et al., 2016) which contains 4,737 questions paired with SPARQL queries for Freebase\n",
    "\n",
    "1. sample question-query pairs\n",
    "2. automatically create more complex SPARQL queries (conjunctions, superlatives, comparatives, and compositions)\n",
    "3. generate automatically questions (MG) that are understandable to Amazon Mechanical Turk (AMT) workers\n",
    "4. have AMT workers paraphrase MG questions into natural language (workers incentivized to create questions with high edit distance compared to the MG question)\n",
    "\n",
    "Each of our examples contains a question, an answer, a SPARQL query (that our models ignore), and all web snippets harvested by our model when attempting to answer the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed model (simplified)\n",
    "\n",
    "1. Use Seq2SQL type model to convert query into a tree as mentioned before\n",
    "    * Noisy supervision to train the model is generated by heuristically working from the original SPARQL query to the natural language question\n",
    "2. Apply the generated tree ops to the query to get a set of \"simple\" questions\n",
    "3. Pass the questions to a ${\\rm S{\\scriptsize IMP}QA}(.)$ function. \n",
    "    > This model sends the question to Google’s search engine and extracts a distribution over answers from the top-100 web snippets using manually-engineered features. We re-train the model on our data with one new feature: for every question *q* and candidate answer mention in a snippet, we run [RASOR](https://arxiv.org/abs/1611.01436), an RC model by lee et al. (2016), and add the output logit score as a feature. We found that combining the web-facing model of Talmor et al. (2017) and RASOR, resulted in improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* Authors perform a detailed analysis of the model performance on various types of questions. However only performance changes caused tweaks to their proposed model are mentioned. As a baseline, they use a model trained for direct question answering (basically SimpQA on the whole query)\n",
    "* Authors also show good performance on the original ComplexQuestions dataset. Compared with a model trained directly on the source database (FreeBase), the authors claim that the proposed architecture performs comparably.\n",
    "\n",
    "**Observations:**\n",
    "* The Seq2SQL model is trained in a constrained manner and can only perform one operation (out of Comp, Conj and SimpQA) for query decomposition. This is an artifact from data generation because the authors know that the questions were made from atmost 2 decomposable sub-questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
